home <- Sys.getenv("SPARK_HOME")
.libPaths(c(file.path(home, "R", "lib"), .libPaths()))
Sys.setenv(NOAWT = 1)

# Make sure SparkR package is the last loaded one
old <- getOption("defaultPackages")
options(defaultPackages = c(old, "SparkR"))

spark <- SparkR::sparkR.session()
assign("spark", spark, envir = .GlobalEnv)
sc <- SparkR:::callJStatic("org.apache.spark.sql.api.r.SQLUtils", "getJavaSparkContext", spark)
assign("sc", sc, envir = .GlobalEnv)
sparkVer <- SparkR:::callJMethod(sc, "version")
cat("\nWelcome to")
cat("\n")
cat("      ____              __", "\n")
cat("     / __/__  ___ _____/ /__", "\n")
cat("    _\\ \\/ _ \\/ _ `/ __/  '_/", "\n")
cat("   /___/ .__/\\_,_/_/ /_/\\_\\")
if (nchar(sparkVer) == 0) {
cat("\n")
} else {
cat("   version", sparkVer, "\n")
}
cat("      /_/", "\n")
cat("\n")

cat("\nSparkSession available as 'spark'.\n")

spark.read.path <- function(path, ...) {
  sparkSession <- SparkR:::getSparkSession()
  updateAccessToken(sparkSession)
  options <- SparkR:::varargsToStrEnv(...)
  options[["path"]] <- path
  read <- SparkR:::callJMethod(sparkSession, "read")
  read <- SparkR:::callJMethod(read, "format", "gsim")
  read <- SparkR:::callJMethod(read, "options", options)
  sdf <- SparkR:::handledCallJMethod(read, "load")
  SparkR:::dataFrame(sdf)
}

spark.write.path <- function(df, path, mode = "overwrite", partitionBy = NULL, ...) {
  sparkSession <- SparkR:::getSparkSession()
  updateAccessToken(sparkSession)
  SparkR:::write.df(df, path, "gsim", mode, partitionBy, ...)
}

updateAccessToken <- function(sparkSession) {
    library(reticulate)
    authextension <- import("dapla.jupyterextensions.authextension")
    tryCatch({
            accessToken = authextension$AuthClient$get_access_token()
        },
        error = function(error_message) {
            IRdisplay::display_html('Your session has timed out. Please <a href="/hub/login">log in</a> to continue.')
            stop("Could not get access token due to session timeout")
        }
    )

    conf = list(spark.ssb.access=accessToken)

    configMap <- SparkR:::convertNamedListToEnv(conf)
    SparkR:::callJStatic("org.apache.spark.sql.api.r.SQLUtils",
                    "setSparkContextSessionConf",
                    sparkSession,
                    configMap)
}

cat("\nCustom spark functions loaded.\n")


